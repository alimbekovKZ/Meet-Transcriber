console.log("üìå Google Meet Transcription Plugin –∑–∞–≥—Ä—É–∂–µ–Ω");

// Global variables
let audioContext;
let mediaRecorder;
let audioChunks = [];
let isRecording = false;
let meetingObserver = null;
let autoTranscriptionEnabled = true;

// Initialize when page loads
window.addEventListener('load', () => {
    // Check if we're on a Google Meet page
    if (window.location.href.includes('meet.google.com')) {
        console.log("üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ Google Meet");
        initializeMeetDetection();
        
        // Check if auto-transcription is enabled in settings
        chrome.storage.local.get(['autoTranscription'], (result) => {
            if (result.hasOwnProperty('autoTranscription')) {
                autoTranscriptionEnabled = result.autoTranscription;
            }
        });
    }
});

// Initialize meeting detection
function initializeMeetDetection() {
    // Look for meeting UI elements to detect when call starts
    meetingObserver = new MutationObserver((mutations) => {
        mutations.forEach((mutation) => {
            if (mutation.addedNodes.length) {
                // Check for video/audio elements that indicate a call has started
                const callStarted = document.querySelector('[data-call-started]') || 
                                    document.querySelector('[data-meeting-active]') ||
                                    document.querySelectorAll('video').length > 0;
                
                if (callStarted && autoTranscriptionEnabled && !isRecording) {
                    console.log("üéâ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–∞—á–∞–ª–æ –∑–≤–æ–Ω–∫–∞ –≤ Google Meet");
                    startRecording();
                }
                
                // Check for meeting name to use in filename
                const meetingNameElement = document.querySelector('[data-meeting-title]') || 
                                          document.querySelector('.r6xAKc');
                if (meetingNameElement) {
                    window.meetingName = meetingNameElement.textContent.trim();
                }
            }
        });
    });
    
    // Start observing document body for changes
    meetingObserver.observe(document.body, { childList: true, subtree: true });
    
    // Also detect page unload to stop recording
    window.addEventListener('beforeunload', () => {
        if (isRecording) {
            stopRecording();
        }
    });
}

// Get audio stream from meeting
async function getAudioStream() {
    console.log("üéß –ü–µ—Ä–µ—Ö–≤–∞—Ç –∞—É–¥–∏–æ: –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–æ—Å—Ç—É–ø...");

    try {
        // Try to capture tab audio first (requires tabCapture permission)
        if (navigator.mediaDevices.getDisplayMedia) {
            const displayStream = await navigator.mediaDevices.getDisplayMedia({
                video: true,
                audio: true
            });
            
            // If we have audio tracks, use them
            const audioTracks = displayStream.getAudioTracks();
            if (audioTracks.length > 0) {
                console.log("‚úÖ –ê—É–¥–∏–æ–ø–æ—Ç–æ–∫ –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ getDisplayMedia:", audioTracks.length, "—Ç—Ä–µ–∫–æ–≤");
                
                // Stop video tracks as we only need audio
                displayStream.getVideoTracks().forEach(track => track.stop());
                
                // Create a new stream with only audio tracks
                const audioStream = new MediaStream(audioTracks);
                return audioStream;
            }
            
            // If no audio tracks, stop the display capture
            displayStream.getTracks().forEach(track => track.stop());
        }
        
        // Fallback to microphone audio
        const micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        console.log("‚úÖ –ê—É–¥–∏–æ–ø–æ—Ç–æ–∫ –ø–æ–ª—É—á–µ–Ω —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞:", micStream);
        return micStream;
    } catch (err) {
        console.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫–∞:", err);
        return null;
    }
}

// Start recording audio
async function startRecording() {
    console.log("üéô –ó–∞–ø—É—Å–∫ –∑–∞–ø–∏—Å–∏...");
    
    if (isRecording) {
        console.log("‚ö†Ô∏è –ó–∞–ø–∏—Å—å —É–∂–µ –∏–¥–µ—Ç");
        return;
    }

    // Initialize AudioContext if needed
    if (!audioContext) {
        audioContext = new AudioContext();
    }

    if (audioContext.state === "suspended") {
        await audioContext.resume();
        console.log("üîä AudioContext –≤–æ–∑–æ–±–Ω–æ–≤–ª—ë–Ω!");
    }

    // Get audio stream
    const stream = await getAudioStream();
    if (!stream) {
        console.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫");
        return;
    }

    // Reset audio chunks
    audioChunks = [];
    
    // Create MediaRecorder
    mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
    });
    
    // Handle audio data
    mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
            audioChunks.push(event.data);
        }
    };

    // Start recording
    mediaRecorder.start(1000); // Capture in 1-second chunks
    isRecording = true;
    
    // Notify background script that recording started
    chrome.runtime.sendMessage({
        type: "recordingStatus",
        status: "started",
        meetingName: window.meetingName || "Unknown Meeting"
    });
    
    console.log("‚ñ∂ –ó–∞–ø–∏—Å—å –Ω–∞—á–∞–ª–∞—Å—å! –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:", mediaRecorder.state);
}

// Stop recording and process audio
// Improved audio recording and processing
async function stopRecording() {
    console.log("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–ø–∏—Å–∏...");

    if (!isRecording || !mediaRecorder || mediaRecorder.state === "inactive") {
        console.log("‚ö†Ô∏è –ó–∞–ø–∏—Å—å –Ω–µ –∞–∫—Ç–∏–≤–Ω–∞, –Ω–µ—á–µ–≥–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å");
        return;
    }

    // Change recording state
    isRecording = false;
    
    // Create a promise to handle the stop event
    const stopPromise = new Promise((resolve) => {
        mediaRecorder.onstop = async () => {
            try {
                // Convert audio chunks to blob
                const audioBlob = new Blob(audioChunks, { 
                    type: mediaRecorder.mimeType || "audio/webm;codecs=opus" 
                });
                console.log("üíæ –ê—É–¥–∏–æ-—Ñ–∞–π–ª —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω:", audioBlob.size, "–±–∞–π—Ç");
                
                // Try to convert to WAV first
                let finalBlob;
                try {
                    finalBlob = await convertToWav(audioBlob);
                    console.log("‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ WAV —É—Å–ø–µ—à–Ω–∞");
                } catch (error) {
                    console.error("‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ WAV:", error);
                    // Fallback to original format with different mime type
                    finalBlob = new Blob([await audioBlob.arrayBuffer()], { type: 'audio/wav' });
                    console.log("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å WAV mime-type");
                }
                
                console.log("üì¶ –§–∏–Ω–∞–ª—å–Ω—ã–π –∞—É–¥–∏–æ—Ñ–∞–π–ª:", finalBlob.size, "–±–∞–π—Ç, —Ç–∏–ø:", finalBlob.type);
                
                // Send to background script for processing
                const reader = new FileReader();
                reader.readAsDataURL(finalBlob); 
                reader.onloadend = function() {
                    chrome.runtime.sendMessage({
                        type: "sendAudioToWhisper",
                        file: reader.result,
                        meetingName: window.meetingName || "Unknown Meeting",
                        format: finalBlob.type
                    }, (response) => {
                        if (chrome.runtime.lastError) {
                            console.error("‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è:", chrome.runtime.lastError.message);
                        } else {
                            console.log("‚úÖ –û—Ç–≤–µ—Ç –æ—Ç background.js:", response);
                        }
                    });
                };
            } catch (error) {
                console.error("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ:", error);
                chrome.runtime.sendMessage({
                    type: "recordingError",
                    error: error.message
                });
            } finally {
                // Release used media tracks
                if (mediaRecorder.stream) {
                    mediaRecorder.stream.getTracks().forEach(track => track.stop());
                }
                
                resolve();
            }
        };
    });
    
    // Stop the recording
    mediaRecorder.stop();
    
    // Notify background script that recording stopped
    chrome.runtime.sendMessage({
        type: "recordingStatus",
        status: "stopped"
    });
    
    // Wait for stop to complete
    await stopPromise;
    console.log("‚èπ –ó–∞–ø–∏—Å—å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É");
}

// Convert WebM to WAV (simplified approach)
// Proper WebM to WAV conversion function
async function convertToWav(webmBlob) {
    console.log("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ –∏–∑ WebM –≤ WAV —Ñ–æ—Ä–º–∞—Ç...");
    
    try {
        // Create audio context
        const audioContext = new AudioContext();
        
        // Read the blob as ArrayBuffer
        const arrayBuffer = await webmBlob.arrayBuffer();
        
        // Decode the audio data
        const audioData = await audioContext.decodeAudioData(arrayBuffer);
        
        // Create a buffer source
        const source = audioContext.createBufferSource();
        source.buffer = audioData;
        
        // Create offline context for rendering
        const offlineCtx = new OfflineAudioContext(
            audioData.numberOfChannels,
            audioData.length,
            audioData.sampleRate
        );
        
        // Create buffer source for offline context
        const offlineSource = offlineCtx.createBufferSource();
        offlineSource.buffer = audioData;
        offlineSource.connect(offlineCtx.destination);
        offlineSource.start();
        
        // Render audio
        const renderedBuffer = await offlineCtx.startRendering();
        
        // Convert to WAV format
        const wavBlob = audioBufferToWav(renderedBuffer);
        
        console.log("‚úÖ –ê—É–¥–∏–æ —É—Å–ø–µ—à–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ WAV —Ñ–æ—Ä–º–∞—Ç");
        return wavBlob;
    } catch (error) {
        console.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∞—É–¥–∏–æ:", error);
        
        // Fallback: Try to send original blob with proper MIME type
        console.log("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∞—É–¥–∏–æ—Ñ–∞–π–ª —Å –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–º MIME —Ç–∏–ø–æ–º");
        return new Blob([await webmBlob.arrayBuffer()], { type: 'audio/wav' });
    }
}

// Function to convert AudioBuffer to WAV Blob
function audioBufferToWav(buffer) {
    const numOfChan = buffer.numberOfChannels;
    const length = buffer.length * numOfChan * 2;
    const sampleRate = buffer.sampleRate;
    
    // Create DataView for WAV header
    const wavDataView = new DataView(new ArrayBuffer(44));
    
    // Write "RIFF" identifier
    writeString(wavDataView, 0, 'RIFF');
    // Write RIFF chunk length
    wavDataView.setUint32(4, 36 + length, true);
    // Write "WAVE" format
    writeString(wavDataView, 8, 'WAVE');
    // Write "fmt " chunk identifier
    writeString(wavDataView, 12, 'fmt ');
    // Write fmt chunk length
    wavDataView.setUint32(16, 16, true);
    // Write format code (1 for PCM)
    wavDataView.setUint16(20, 1, true);
    // Write number of channels
    wavDataView.setUint16(22, numOfChan, true);
    // Write sample rate
    wavDataView.setUint32(24, sampleRate, true);
    // Write byte rate (sample rate * block align)
    wavDataView.setUint32(28, sampleRate * numOfChan * 2, true);
    // Write block align (num of channels * bits per sample / 8)
    wavDataView.setUint16(32, numOfChan * 2, true);
    // Write bits per sample
    wavDataView.setUint16(34, 16, true);
    // Write "data" chunk identifier
    writeString(wavDataView, 36, 'data');
    // Write data chunk length
    wavDataView.setUint32(40, length, true);
    
    // Create the final buffer with header and audio data
    const wavData = new DataView(new ArrayBuffer(44 + length));
    
    // Copy WAV header
    for (let i = 0; i < 44; i++) {
        wavData.setUint8(i, wavDataView.getUint8(i));
    }
    
    // Convert audio data to 16-bit PCM and write it
    let offset = 44;
    for (let i = 0; i < buffer.numberOfChannels; i++) {
        const channelData = buffer.getChannelData(i);
        for (let j = 0; j < channelData.length; j++) {
            // Scale float32 to int16
            const sample = Math.max(-1, Math.min(1, channelData[j]));
            const int16Sample = sample < 0 
                ? sample * 0x8000 
                : sample * 0x7FFF;
            
            wavData.setInt16(offset, int16Sample, true);
            offset += 2;
        }
    }
    
    // Create WAV blob
    return new Blob([wavData], { type: 'audio/wav' });
}

// Helper function to write strings to DataView
function writeString(dataView, offset, string) {
    for (let i = 0; i < string.length; i++) {
        dataView.setUint8(offset + i, string.charCodeAt(i));
    }
}

// Disable auto-transcription for current meeting
function disableAutoTranscription() {
    autoTranscriptionEnabled = false;
    
    if (isRecording) {
        stopRecording();
    }
    
    console.log("üîï –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–π –≤—Å—Ç—Ä–µ—á–∏");
}

// Listen for messages from popup.js
chrome.runtime.onMessage.addListener((message, sender, sendResponse) => {
    if (message.action === "startRecording") {
        console.log("üì© –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ 'startRecording'");
        startRecording();
        sendResponse({ status: "‚úÖ –ó–∞–ø–∏—Å—å –Ω–∞—á–∞–ª–∞—Å—å!" });
    }
    else if (message.action === "stopRecording") {
        console.log("üì© –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ 'stopRecording'");
        stopRecording();
        sendResponse({ status: "‚úÖ –ó–∞–ø–∏—Å—å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!" });
    }
    else if (message.action === "disableAutoTranscription") {
        console.log("üì© –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ 'disableAutoTranscription'");
        disableAutoTranscription();
        sendResponse({ status: "‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞!" });
    }
    else if (message.action === "getRecordingStatus") {
        sendResponse({ 
            isRecording: isRecording,
            meetingDetected: !!window.meetingName,
            meetingName: window.meetingName || "Unknown Meeting"
        });
    }
    
    return true; // Important for asynchronous sendResponse
});