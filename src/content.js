console.log("üìå Google Meet Transcription Plugin –∑–∞–≥—Ä—É–∂–µ–Ω");

// Global variables
let audioContext;
let mediaRecorder;
let audioChunks = [];
let isRecording = false;
let meetingObserver = null;
let autoTranscriptionEnabled = true;

// Initialize when page loads
window.addEventListener('load', () => {
    // Check if we're on a Google Meet page
    if (window.location.href.includes('meet.google.com')) {
        console.log("üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ Google Meet");
        initializeMeetDetection();
        
        // Check if auto-transcription is enabled in settings
        chrome.storage.local.get(['autoTranscription'], (result) => {
            if (result.hasOwnProperty('autoTranscription')) {
                autoTranscriptionEnabled = result.autoTranscription;
            }
        });
    }
});

// Initialize meeting detection
function initializeMeetDetection() {
    // Look for meeting UI elements to detect when call starts
    meetingObserver = new MutationObserver((mutations) => {
        mutations.forEach((mutation) => {
            if (mutation.addedNodes.length) {
                // Check for video/audio elements that indicate a call has started
                const callStarted = document.querySelector('[data-call-started]') || 
                                    document.querySelector('[data-meeting-active]') ||
                                    document.querySelectorAll('video').length > 0;
                
                if (callStarted && autoTranscriptionEnabled && !isRecording) {
                    console.log("üéâ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–∞—á–∞–ª–æ –∑–≤–æ–Ω–∫–∞ –≤ Google Meet");
                    startRecording();
                }
                
                // Check for meeting name to use in filename
                const meetingNameElement = document.querySelector('[data-meeting-title]') || 
                                          document.querySelector('.r6xAKc');
                if (meetingNameElement) {
                    window.meetingName = meetingNameElement.textContent.trim();
                }
            }
        });
    });
    
    // Start observing document body for changes
    meetingObserver.observe(document.body, { childList: true, subtree: true });
    
    // Also detect page unload to stop recording
    window.addEventListener('beforeunload', () => {
        if (isRecording) {
            stopRecording();
        }
    });
}

// Get audio stream from meeting
async function getAudioStream() {
    console.log("üéß –ü–µ—Ä–µ—Ö–≤–∞—Ç –∞—É–¥–∏–æ: –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–æ—Å—Ç—É–ø...");

    try {
        // Try to capture tab audio first (requires tabCapture permission)
        if (navigator.mediaDevices.getDisplayMedia) {
            const displayStream = await navigator.mediaDevices.getDisplayMedia({
                video: true,
                audio: true
            });
            
            // If we have audio tracks, use them
            const audioTracks = displayStream.getAudioTracks();
            if (audioTracks.length > 0) {
                console.log("‚úÖ –ê—É–¥–∏–æ–ø–æ—Ç–æ–∫ –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ getDisplayMedia:", audioTracks.length, "—Ç—Ä–µ–∫–æ–≤");
                
                // Stop video tracks as we only need audio
                displayStream.getVideoTracks().forEach(track => track.stop());
                
                // Create a new stream with only audio tracks
                const audioStream = new MediaStream(audioTracks);
                return audioStream;
            }
            
            // If no audio tracks, stop the display capture
            displayStream.getTracks().forEach(track => track.stop());
        }
        
        // Fallback to microphone audio
        const micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        console.log("‚úÖ –ê—É–¥–∏–æ–ø–æ—Ç–æ–∫ –ø–æ–ª—É—á–µ–Ω —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞:", micStream);
        return micStream;
    } catch (err) {
        console.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫–∞:", err);
        return null;
    }
}

// Start recording audio
async function startRecording() {
    console.log("üéô –ó–∞–ø—É—Å–∫ –∑–∞–ø–∏—Å–∏...");
    
    if (isRecording) {
        console.log("‚ö†Ô∏è –ó–∞–ø–∏—Å—å —É–∂–µ –∏–¥–µ—Ç");
        return;
    }

    // Initialize AudioContext if needed
    if (!audioContext) {
        audioContext = new AudioContext();
    }

    if (audioContext.state === "suspended") {
        await audioContext.resume();
        console.log("üîä AudioContext –≤–æ–∑–æ–±–Ω–æ–≤–ª—ë–Ω!");
    }

    // Get audio stream
    const stream = await getAudioStream();
    if (!stream) {
        console.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫");
        return;
    }

    // Reset audio chunks
    audioChunks = [];
    
    // Create MediaRecorder
    mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
    });
    
    // Handle audio data
    mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
            audioChunks.push(event.data);
        }
    };

    // Start recording
    mediaRecorder.start(1000); // Capture in 1-second chunks
    isRecording = true;
    
    // Notify background script that recording started
    chrome.runtime.sendMessage({
        type: "recordingStatus",
        status: "started",
        meetingName: window.meetingName || "Unknown Meeting"
    });
    
    console.log("‚ñ∂ –ó–∞–ø–∏—Å—å –Ω–∞—á–∞–ª–∞—Å—å! –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:", mediaRecorder.state);
}

// Stop recording and process audio
async function stopRecording() {
    console.log("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–ø–∏—Å–∏...");

    if (!isRecording || !mediaRecorder || mediaRecorder.state === "inactive") {
        console.log("‚ö†Ô∏è –ó–∞–ø–∏—Å—å –Ω–µ –∞–∫—Ç–∏–≤–Ω–∞, –Ω–µ—á–µ–≥–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å");
        return;
    }

    // Change recording state
    isRecording = false;
    
    // Create a promise to handle the stop event
    const stopPromise = new Promise((resolve) => {
        mediaRecorder.onstop = async () => {
            // Convert audio chunks to blob
            const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
            console.log("üíæ –ê—É–¥–∏–æ-—Ñ–∞–π–ª —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω:", audioBlob.size, "–±–∞–π—Ç");
            
            // Convert to WAV for better compatibility with Whisper API
            const wavBlob = await convertToWav(audioBlob);
            
            // Send to background script for processing
            const reader = new FileReader();
            reader.readAsDataURL(wavBlob); 
            reader.onloadend = function() {
                chrome.runtime.sendMessage({
                    type: "sendAudioToWhisper",
                    file: reader.result,
                    meetingName: window.meetingName || "Unknown Meeting"
                }, (response) => {
                    if (chrome.runtime.lastError) {
                        console.error("‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è:", chrome.runtime.lastError.message);
                    } else {
                        console.log("‚úÖ –û—Ç–≤–µ—Ç –æ—Ç background.js:", response);
                    }
                });
            };
            
            // Release used media tracks
            if (mediaRecorder.stream) {
                mediaRecorder.stream.getTracks().forEach(track => track.stop());
            }
            
            resolve();
        };
    });
    
    // Stop the recording
    mediaRecorder.stop();
    
    // Notify background script that recording stopped
    chrome.runtime.sendMessage({
        type: "recordingStatus",
        status: "stopped"
    });
    
    // Wait for stop to complete
    await stopPromise;
    console.log("‚èπ –ó–∞–ø–∏—Å—å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É");
}

// Convert WebM to WAV (simplified approach)
async function convertToWav(webmBlob) {
    // In a real implementation, we would use audio libraries to convert
    // For this example, we'll just return the original blob
    // In production, you'd want to use WebAudio API to properly convert
    console.log("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ —Ñ–æ—Ä–º–∞—Ç–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)");
    return webmBlob;
}

// Disable auto-transcription for current meeting
function disableAutoTranscription() {
    autoTranscriptionEnabled = false;
    
    if (isRecording) {
        stopRecording();
    }
    
    console.log("üîï –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–π –≤—Å—Ç—Ä–µ—á–∏");
}

// Listen for messages from popup.js
chrome.runtime.onMessage.addListener((message, sender, sendResponse) => {
    if (message.action === "startRecording") {
        console.log("üì© –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ 'startRecording'");
        startRecording();
        sendResponse({ status: "‚úÖ –ó–∞–ø–∏—Å—å –Ω–∞—á–∞–ª–∞—Å—å!" });
    }
    else if (message.action === "stopRecording") {
        console.log("üì© –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ 'stopRecording'");
        stopRecording();
        sendResponse({ status: "‚úÖ –ó–∞–ø–∏—Å—å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!" });
    }
    else if (message.action === "disableAutoTranscription") {
        console.log("üì© –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ 'disableAutoTranscription'");
        disableAutoTranscription();
        sendResponse({ status: "‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞!" });
    }
    else if (message.action === "getRecordingStatus") {
        sendResponse({ 
            isRecording: isRecording,
            meetingDetected: !!window.meetingName,
            meetingName: window.meetingName || "Unknown Meeting"
        });
    }
    
    return true; // Important for asynchronous sendResponse
});